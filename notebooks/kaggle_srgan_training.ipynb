{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8023940,"sourceType":"datasetVersion","datasetId":4728542}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-06T16:05:27.597669Z","iopub.execute_input":"2024-04-06T16:05:27.598049Z","iopub.status.idle":"2024-04-06T16:05:29.354226Z","shell.execute_reply.started":"2024-04-06T16:05:27.598019Z","shell.execute_reply":"2024-04-06T16:05:29.353240Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/input/div2k-data/DIV2K_valid_HR/DIV2K_valid_HR/0857.png\n/kaggle/input/div2k-data/DIV2K_train_HR/DIV2K_train_HR/0566.png\n/kaggle/input/div2k-data/DIV2K_valid_LR_difficult/DIV2K_valid_LR_difficult/0888x4d.png\n/kaggle/input/div2k-data/DIV2K_train_LR_difficult/DIV2K_train_LR_difficult/0087x4d.png\n","output_type":"stream"}]},{"cell_type":"code","source":"## generator_loss.py\nimport torch\nfrom torch import nn\nfrom torchvision.models.vgg import vgg16\n\nclass TVLoss(nn.Module):\n    def __init__(self, tv_loss_weight=1):\n        super(TVLoss, self).__init__()\n        self.tv_loss_weight = tv_loss_weight\n\n    def forward(self, x):\n        batch_size, c_x, h_x, w_x = x.size()\n        count_h = self.tensor_size(x[:, :, 1:, :])\n        count_w = self.tensor_size(x[:, :, :, 1:])\n        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n\n    @staticmethod\n    def tensor_size(t):\n        return t.size()[1] * t.size()[2] * t.size()[3]\n\nclass GeneratorLoss(nn.Module):\n    def __init__(self):\n        super(GeneratorLoss, self).__init__()\n        vgg = vgg16(pretrained=True)\n        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n        for param in loss_network.parameters():\n            param.requires_grad = False\n        self.loss_network = loss_network\n        self.mse_loss = nn.MSELoss()\n        self.tv_loss = TVLoss()\n\n    def forward(self, out_labels, out_images, target_images):\n        # Adversarial Loss\n        adversarial_loss = torch.mean(1 - out_labels)\n        # Perception Loss\n        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n        # Image Loss\n        image_loss = self.mse_loss(out_images, target_images)\n        # TV Loss\n        tv_loss = self.tv_loss(out_images)\n        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss","metadata":{"execution":{"iopub.status.busy":"2024-04-06T16:05:29.355664Z","iopub.execute_input":"2024-04-06T16:05:29.356080Z","iopub.status.idle":"2024-04-06T16:05:35.484860Z","shell.execute_reply.started":"2024-04-06T16:05:29.356054Z","shell.execute_reply":"2024-04-06T16:05:35.484076Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"## generator.py\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ganGenerator(nn.Module):\n    def __init__(self):\n        super(ganGenerator, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=9, stride=1, padding=4)\n        self.prelu1 = nn.PReLU()\n        self.GRB1 = GeneratorResidualBlock()\n        self.GRB2 = GeneratorResidualBlock()\n        self.GRB3 = GeneratorResidualBlock()\n        self.GRB4 = GeneratorResidualBlock()\n        self.GRB5 = GeneratorResidualBlock()\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, stride=1, padding=1)\n        self.pxlshuffle1 = nn.PixelShuffle(2)\n        self.prelu2 = nn.PReLU()\n        self.conv4 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, stride=1, padding=1)\n        self.pxlshuffle2 = nn.PixelShuffle(2)\n        self.prelu3 = nn.PReLU()\n        self.conv5 = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=9, stride=1, padding=4)\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x1 = self.prelu1(x1)\n\n        x2 = self.GRB1(x1)\n        x2 = self.GRB2(x2)\n        x2 = self.GRB3(x2)\n        x2 = self.GRB4(x2)\n        x2 = self.GRB5(x2)\n\n        x2 = self.conv2(x2)\n        x2 = self.bn1(x2)\n        x3 = x1 + x2\n\n        x3 = self.conv3(x3)\n        x3 = self.pxlshuffle1(x3)\n        x3 = self.prelu2(x3)\n        x3 = self.conv4(x3)\n        x3 = self.pxlshuffle2(x3)\n        x4 = self.prelu3(x3)\n\n        x5 = self.conv5(x4)\n\n        return x5\n\nclass GeneratorResidualBlock(nn.Module):\n    def __init__(self):\n        super(GeneratorResidualBlock, self).__init__()\n        # convolution\n        self.conv1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n        # batchnorm\n        self.bn1 = nn.BatchNorm2d(64)\n        # prelu\n        self.prelu1 = nn.PReLU()\n        #convolution\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n        # batchnorm\n        self.bn2 = nn.BatchNorm2d(64)\n        # prelu\n        self.prelu2 = nn.PReLU()\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.prelu1(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.prelu2(out)\n        return out + x","metadata":{"execution":{"iopub.status.busy":"2024-04-06T16:05:35.486428Z","iopub.execute_input":"2024-04-06T16:05:35.486863Z","iopub.status.idle":"2024-04-06T16:05:35.503375Z","shell.execute_reply.started":"2024-04-06T16:05:35.486838Z","shell.execute_reply":"2024-04-06T16:05:35.502379Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"## discriminator.py\nimport torch.nn as nn\n\nclass ganDiscriminator(nn.Module):\n    def __init__(self):\n        super(ganDiscriminator, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1)\n        self.lrelu1 = nn.LeakyReLU()\n\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.lrelu2 = nn.LeakyReLU()\n\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.lrelu3 = nn.LeakyReLU()\n        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2)\n        self.bn4 = nn.BatchNorm2d(128)\n        self.lrelu4 = nn.LeakyReLU()\n\n        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1)\n        self.bn5 = nn.BatchNorm2d(256)\n        self.lrelu5 = nn.LeakyReLU()\n        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2)\n        self.bn6 = nn.BatchNorm2d(256)\n        self.lrelu6 = nn.LeakyReLU()\n\n        self.conv7 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1)\n        self.bn7 = nn.BatchNorm2d(512)\n        self.lrelu7 = nn.LeakyReLU()\n        self.conv8 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=2)\n        self.bn8 = nn.BatchNorm2d(512)\n        self.lrelu8 = nn.LeakyReLU()\n\n        # self.flat = nn.Flatten()\n\n        # self.dense9 = nn.Linear(in_features=60*124*512, out_features=1024, bias=True)\n        # self.lrelu9 = nn.LeakyReLU()\n\n        # self.dense10 = nn.Linear(in_features=1024, out_features=1, bias=True)\n        # self.sigmoid10 = nn.Sigmoid()\n\n        self.adaptive_pool_1 = nn.AdaptiveAvgPool2d(1)\n        self.conv9 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1)\n        self.lrelu9 = nn.LeakyReLU()\n        self.conv10 = nn.Conv2d(in_channels=1024, out_channels=1, kernel_size=1) \n        self.sigmoid9 = nn.Sigmoid()\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x1 = self.lrelu1(x1)\n\n        x2 = self.conv2(x1)\n        x2 = self.bn2(x2)\n        x2 = self.lrelu2(x2)\n\n        x3 = self.conv3(x2)\n        x3 = self.bn3(x3)\n        x3 = self.lrelu3(x3)\n\n        x4 = self.conv4(x3)\n        x4 = self.bn4(x4)\n        x4 = self.lrelu4(x4)\n\n        x5 = self.conv5(x4)\n        x5 = self.bn5(x5)\n        x5 = self.lrelu5(x5)\n\n        x6 = self.conv6(x5)\n        x6 = self.bn6(x6)\n        x6 = self.lrelu6(x6)\n\n        x7 = self.conv7(x6)\n        x7 = self.bn7(x7)\n        x7 = self.lrelu7(x7)\n\n        x8 = self.conv8(x7)\n        x8 = self.bn8(x8)\n        x8 = self.lrelu8(x8)\n\n        x9 = self.adaptive_pool_1(x8)\n        x9 = self.conv9(x9)\n        x9 = self.lrelu9(x9)\n\n        x10 = self.conv10(x9)\n        x10 = self.sigmoid9(x10.view(x10.size()[0]))\n\n        # x9 = self.flat(x8)\n        # x10 = self.dense9(x9)\n        # x10 = self.lrelu9(x10)\n        # x11 = self.dense10(x10)\n        # x11 = self.sigmoid10(x11)\n\n        x11 = x10\n\n        return x11","metadata":{"execution":{"iopub.status.busy":"2024-04-06T16:05:35.504720Z","iopub.execute_input":"2024-04-06T16:05:35.505012Z","iopub.status.idle":"2024-04-06T16:05:35.523659Z","shell.execute_reply.started":"2024-04-06T16:05:35.504987Z","shell.execute_reply":"2024-04-06T16:05:35.522758Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"## data_utils.py\nfrom PIL import Image\nfrom torchvision.transforms import Compose, ToTensor, ToPILImage, CenterCrop, transforms, Resize\nfrom torch.utils.data.dataset import Dataset\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n\nhr_target_size = (1020, 2040)\n\nhr_transform = transforms.Compose([\n    transforms.Lambda(lambda img: img.rotate(0) if img.size[0] > img.size[1] else img.rotate(90)),\n    transforms.CenterCrop(hr_target_size),\n    transforms.ToTensor()\n])\n\nlr_target_size = (1020 // 4, 2040 // 4)\n\nlr_transform = transforms.Compose([\n    transforms.Lambda(lambda img: img.rotate(0) if img.size[0] > img.size[1] else img.rotate(90)),\n    transforms.CenterCrop(lr_target_size),\n    transforms.ToTensor()\n])\n\n\nclass Div2kTrainDataset(Dataset):\n    def __init__(self, hr_base_dir, lr_base_dir):\n        super(Div2kTrainDataset, self).__init__()\n\n        self.hr_base_dir = hr_base_dir\n        self.lr_base_dir = lr_base_dir\n\n        self.hr_image_filenames = [f'{self.hr_base_dir}/{i:0>4}.png' for i in range(1, 801)]\n        self.lr_image_filenames = [f'{self.lr_base_dir}/{i:0>4}x4d.png' for i in range(1, 801)]\n\n        self.hr_transform = hr_transform\n        self.lr_transform = lr_transform\n\n    def __getitem__(self, index):\n        hr_image = self.hr_transform(Image.open(self.hr_image_filenames[index]))\n        lr_image = self.lr_transform(Image.open(self.lr_image_filenames[index]))\n        return hr_image, lr_image\n\n    def __len__(self):\n        return len(self.hr_image_filenames)\n\nclass Div2kValDataset(Dataset):\n    def __init__(self, hr_base_dir, lr_base_dir):\n        super(Div2kValDataset, self).__init__()\n\n        self.hr_base_dir = hr_base_dir\n        self.lr_base_dir = lr_base_dir\n\n        self.hr_image_filenames = [f'{self.hr_base_dir}/{i:0>4}.png' for i in range(801, 901)]\n        self.lr_image_filenames = [f'{self.lr_base_dir}/{i:0>4}x4d.png' for i in range(801, 901)]\n\n        self.hr_transform = hr_transform\n        self.lr_transform = lr_transform\n\n    def __getitem__(self, index):\n        hr_image = self.hr_transform(Image.open(self.hr_image_filenames[index]))\n        lr_image = self.lr_transform(Image.open(self.lr_image_filenames[index]))\n        return hr_image, lr_image\n\n    def __len__(self):\n        return len(self.hr_image_filenames)\n    \ndef display_transform():\n    return Compose([\n        ToPILImage(),\n        Resize(400),\n        CenterCrop(400),\n        ToTensor()\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-04-06T16:05:35.525411Z","iopub.execute_input":"2024-04-06T16:05:35.525710Z","iopub.status.idle":"2024-04-06T16:05:35.542774Z","shell.execute_reply.started":"2024-04-06T16:05:35.525678Z","shell.execute_reply":"2024-04-06T16:05:35.541851Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"## train.py\n# aliased imports\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.utils as utils\n\n# package imports no alias\nimport torch\nimport os\n# import pytorch_ssim\nimport math\n\n# Class/Fuction Imports\nfrom tqdm import tqdm\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\n# Local Imports\n# from generator import ganGenerator\n# from discriminator import ganDiscriminator\n# from generator_loss import GeneratorLoss\n# from data_utils import Div2kValDataset, Div2kTrainDataset, display_transform\n\n# filter warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain_hr_loc = \"/kaggle/input/div2k-data/DIV2K_train_HR/DIV2K_train_HR\"\ntrain_lr_loc = \"/kaggle/input/div2k-data/DIV2K_train_LR_difficult/DIV2K_train_LR_difficult\"\nvalid_hr_loc = \"/kaggle/input/div2k-data/DIV2K_valid_HR/DIV2K_valid_HR\"\nvalid_lr_loc = \"/kaggle/input/div2k-data/DIV2K_valid_LR_difficult/DIV2K_valid_LR_difficult\"\n\nif __name__ == '__main__':\n\n    NUM_EPOCHS = 40\n\n    train_set = Div2kTrainDataset(train_hr_loc, train_lr_loc)\n    val_set = Div2kValDataset(valid_hr_loc, valid_lr_loc)\n    train_loader = DataLoader(dataset=train_set, batch_size=1, shuffle=True)\n    val_loader = DataLoader(dataset=val_set, batch_size=1, shuffle=False)\n\n\n    netG = ganGenerator()\n    netG.load_state_dict(torch.load(\"/kaggle/working/epochs/netG_epoch_4_26.pth\"), strict=False)\n    print('# generator parameters:', sum(param.numel() for param in netG.parameters()))\n    netD = ganDiscriminator()\n    netD.load_state_dict(torch.load(\"/kaggle/working/epochs/netD_epoch_4_26.pth\"), strict=False)\n    print('# discriminator parameters:', sum(param.numel() for param in netD.parameters()))\n\n    generator_criterion = GeneratorLoss()\n\n    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    device_gen = torch.device(\"cuda:0\")\n    device_disc = torch.device(\"cuda:0\")\n    # if torch.cuda.is_available():\n    #     netG.to(device)\n    #     print(\"netG sent to cuda\")\n    #     netD.to(device)\n    #     print(\"netD sent to cuda\")\n    #     generator_criterion.to(device)\n    #     print(\"generator criterion sent to cuda\")\n\n    if torch.cuda.is_available():\n        netG.to(device_gen)\n        print(\"netG sent to cuda\")\n        netD.to(device_disc)\n        print(\"netD sent to cuda\")\n        generator_criterion.to(device_gen)\n        print(\"generator criterion sent to cuda\")\n\n    optimizerG = optim.Adam(netG.parameters())\n    optimizerD = optim.Adam(netD.parameters())\n\n    results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n\n    for epoch in range(27, NUM_EPOCHS + 1):\n        train_bar = tqdm(train_loader)\n        running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n\n        netG.train()\n        netD.train()\n\n        for target, source in train_bar:\n            g_update_first = True\n            batch_size = target.size(0)\n            running_results['batch_sizes'] += batch_size\n\n            ############################\n            # (1) Update D network: maximize D(x)-1-D(G(z))\n            ###########################\n            real_img = Variable(target)\n            if torch.cuda.is_available():\n                real_img = real_img.to(device_disc)\n            z = Variable(source)\n            if torch.cuda.is_available():\n                z = z.to(device_gen)\n            fake_img = netG(z).to(device_disc)\n\n            netD.zero_grad()\n            real_out = netD(real_img).mean()\n            fake_out = netD(fake_img).mean()\n            d_loss = 1 - real_out + fake_out\n            d_loss.backward()\n            optimizerD.step()\n\n            ############################\n            # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n            ###########################\n            netG.zero_grad()\n            ## The two lines below are added to prevent runetime error in Google Colab ##\n#             fake_img = netG(z).detach().to(device_disc)\n            fake_img = netG(z).to(device_disc)\n            fake_out = netD(fake_img).mean().to(device_gen)\n            real_img = real_img.to(device_gen)\n            fake_img = fake_img.to(device_gen)\n            ##\n            g_loss = generator_criterion(fake_out, fake_img, real_img)\n            g_loss.backward()\n\n#             fake_img = netG(z).detach().to(device_disc)\n            fake_img = netG(z).to(device_disc)\n            fake_out = netD(fake_img).mean()\n\n            optimizerG.step()\n\n            # loss for current batch before optimization\n            running_results['g_loss'] += g_loss.item() * batch_size\n            running_results['d_loss'] += d_loss.item() * batch_size\n            running_results['d_score'] += real_out.item() * batch_size\n            running_results['g_score'] += fake_out.item() * batch_size\n\n            train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n                epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n                running_results['g_loss'] / running_results['batch_sizes'],\n                running_results['d_score'] / running_results['batch_sizes'],\n                running_results['g_score'] / running_results['batch_sizes']))\n\n        netG.eval()\n        out_path = '/kaggle/working/training_results/'\n        if not os.path.exists(out_path):\n            os.makedirs(out_path)\n\n        with torch.no_grad():\n            val_bar = tqdm(val_loader)\n            validation_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n            val_images = []\n            for val_hr, val_lr in val_bar:\n                batch_size = val_lr.size(0)\n                validation_results['batch_sizes'] += batch_size\n                lr = val_lr\n                hr = val_hr\n                if torch.cuda.is_available():\n                    lr = lr.to(device_gen)\n                    hr = hr.to(device_gen)\n                sr = netG(lr)\n\n                batch_mse = ((sr - hr) ** 2).data.mean()\n                validation_results['mse'] += batch_mse * batch_size\n                # batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n                # validation_results['ssims'] += batch_ssim * batch_size\n                validation_results['psnr'] = 10 * math.log10((hr.max()**2) / (validation_results['mse'] / validation_results['batch_sizes']))\n                # validation_results['ssim'] = validation_results['ssims'] / validation_results['batch_sizes']\n                val_bar.set_description(\n                    desc='[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n                        validation_results['psnr'], 0))\n\n                val_images.extend(\n                    [display_transform()(lr.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)),\n                        display_transform()(sr.data.cpu().squeeze(0))])\n            val_images = torch.stack(val_images)\n            val_images = torch.chunk(val_images, val_images.size(0) // 15)\n            val_save_bar = tqdm(val_images, desc='[saving training results]')\n            index = 1\n            for image in val_save_bar:\n                image = utils.make_grid(image, nrow=3, padding=5)\n                utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding=5)\n                index += 1\n\n        if not os.path.exists('/kaggle/working/epochs/'):\n            os.makedirs('/kaggle/working/epochs/')\n        # save model parameters\n        torch.save(netG.state_dict(), '/kaggle/working/epochs/netG_epoch_%d_%d.pth' % (4, epoch))\n        torch.save(netD.state_dict(), '/kaggle/working/epochs/netD_epoch_%d_%d.pth' % (4, epoch))\n        # save loss\\scores\\psnr\\ssim\n        results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n        results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n        results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n        results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n        results['psnr'].append(validation_results['psnr'])\n        # results['ssim'].append(validation_results['ssim'])\n\n#         if epoch % 10 == 0 and epoch != 0:\n#             out_path = '/kaggle/working/statistics/'\n#             if not os.path.exists(out_path):\n#                 os.makedirs(out_path)\n#             data_frame = pd.DataFrame(\n#                 data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n#                         'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': 0},\n#                 index=range(1, epoch + 1))\n#             data_frame.to_csv(out_path + 'srf_' + str(4) + '_train_results.csv', index_label='Epoch')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T16:07:23.488500Z","iopub.execute_input":"2024-04-06T16:07:23.489111Z","iopub.status.idle":"2024-04-06T22:56:42.764079Z","shell.execute_reply.started":"2024-04-06T16:07:23.489079Z","shell.execute_reply":"2024-04-06T22:56:42.762615Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"# generator parameters: 734224\n# discriminator parameters: 5215425\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:03<00:00, 172MB/s] \n","output_type":"stream"},{"name":"stdout","text":"netG sent to cuda\nnetD sent to cuda\ngenerator criterion sent to cuda\n","output_type":"stream"},{"name":"stderr","text":"[27/40] Loss_D: 1.0000 Loss_G: 0.0248 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 800/800 [40:32<00:00,  3.04s/it]\n[converting LR images to SR images] PSNR: 16.8992 dB SSIM: 0.0000: 100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n[saving training results]: 100%|██████████| 20/20 [00:25<00:00,  1.30s/it]\n[28/40] Loss_D: 1.0000 Loss_G: 0.0195 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 800/800 [39:30<00:00,  2.96s/it]\n[converting LR images to SR images] PSNR: 17.6777 dB SSIM: 0.0000: 100%|██████████| 100/100 [00:47<00:00,  2.11it/s]\n[saving training results]: 100%|██████████| 20/20 [00:25<00:00,  1.29s/it]\n[29/40] Loss_D: 1.0000 Loss_G: 0.0182 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 800/800 [39:29<00:00,  2.96s/it]\n[converting LR images to SR images] PSNR: 17.5770 dB SSIM: 0.0000: 100%|██████████| 100/100 [00:47<00:00,  2.12it/s]\n[saving training results]: 100%|██████████| 20/20 [00:25<00:00,  1.28s/it]\n[30/40] Loss_D: 1.0000 Loss_G: 0.0187 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 800/800 [39:29<00:00,  2.96s/it]\n[converting LR images to SR images] PSNR: 17.6297 dB SSIM: 0.0000: 100%|██████████| 100/100 [00:47<00:00,  2.11it/s]\n[saving training results]: 100%|██████████| 20/20 [00:25<00:00,  1.29s/it]\n[31/40] Loss_D: 1.0000 Loss_G: 0.0179 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 800/800 [39:28<00:00,  2.96s/it]\n[converting LR images to SR images] PSNR: 17.9095 dB SSIM: 0.0000: 100%|██████████| 100/100 [00:47<00:00,  2.12it/s]\n[saving training results]: 100%|██████████| 20/20 [00:25<00:00,  1.29s/it]\n[32/40] Loss_D: 1.0000 Loss_G: 0.0175 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 800/800 [39:28<00:00,  2.96s/it]\n[converting LR images to SR images] PSNR: 17.7491 dB SSIM: 0.0000: 100%|██████████| 100/100 [00:47<00:00,  2.12it/s]\n[saving training results]: 100%|██████████| 20/20 [00:25<00:00,  1.29s/it]\n[33/40] Loss_D: 1.0000 Loss_G: 0.0174 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 800/800 [39:29<00:00,  2.96s/it]\n[converting LR images to SR images] PSNR: 17.8090 dB SSIM: 0.0000: 100%|██████████| 100/100 [00:47<00:00,  2.12it/s]\n[saving training results]: 100%|██████████| 20/20 [00:25<00:00,  1.30s/it]\n[34/40] Loss_D: 1.0000 Loss_G: 0.0170 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 800/800 [39:27<00:00,  2.96s/it]\n[converting LR images to SR images] PSNR: 17.6967 dB SSIM: 0.0000: 100%|██████████| 100/100 [00:46<00:00,  2.14it/s]\n[saving training results]: 100%|██████████| 20/20 [00:25<00:00,  1.27s/it]\n[35/40] Loss_D: 1.0000 Loss_G: 0.0724 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 800/800 [39:27<00:00,  2.96s/it]\n[converting LR images to SR images] PSNR: 15.4179 dB SSIM: 0.0000: 100%|██████████| 100/100 [00:46<00:00,  2.14it/s]\n[saving training results]: 100%|██████████| 20/20 [00:24<00:00,  1.22s/it]\n[36/40] Loss_D: 1.0000 Loss_G: 0.0224 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 800/800 [39:28<00:00,  2.96s/it]\n[converting LR images to SR images] PSNR: 17.1840 dB SSIM: 0.0000: 100%|██████████| 100/100 [00:46<00:00,  2.13it/s]\n[saving training results]: 100%|██████████| 20/20 [00:25<00:00,  1.28s/it]\n[37/40] Loss_D: 1.0000 Loss_G: 0.0197 D(x): 1.0000 D(G(z)): 1.0000:   3%|▎         | 21/800 [01:05<40:15,  3.10s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 130\u001b[0m\n\u001b[1;32m    127\u001b[0m optimizerG\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# loss for current batch before optimization\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m running_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mg_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[1;32m    131\u001b[0m running_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[1;32m    132\u001b[0m running_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m real_out\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m batch_size\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}